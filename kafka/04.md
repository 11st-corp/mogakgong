# Kafka의 내부 동작 원리와 구현

---

## Replication

높은 가용성을 위해 제공되는 기능으로 각 Topic의 Partition들을 Kafka Cluster 내 다른 Broker로 복제하는 것을 의미한다. Topic을 생성할 때 `--replication-factor` 옵션으로 Replication 수를 지정해야 한다.

![Kafka Replication](https://images.ctfassets.net/gt6dp23g0g38/HZjoaXOuEc1zteyMcoOww/1ebab125a11552f4e8b8d88e7850f0ad/Kafka_Internals_029.png)

생성된 Replication은 `Leader`와 `Follower`로 나뉘며 `ISR (In-Sync Replica)` 이라는 일종의 Replication Group을 형성하여 관리된다.

### Leader / Follower

![Leader/Follower, ISR](https://images.ctfassets.net/gt6dp23g0g38/4Llth82ZvCCBqcHfp7v0lH/60e6f507fdccce263d38b6d285e6b143/Kafka_Internals_030.png)

Leader는 Replication 중 하나가 선정되며 모든 Read / Write Operation이 Leader를 통해서만 수행된다. 다시 말해 Producer와 Consumer의 Message 생성과 읽기는 모두 Leader를 통해서만 이루어진다.

Follower는 Leader에 문제가 발생할 경우를 대비해 새로운 Leader가 될 준비를 해야 한다. 따라서 지속적으로 Leader가 새로운 Message를 받았는지 확인하고 새로운 Message가 있는 경우 Leader로부터 해당 Message를 복제한다.

### 복제 유지와 커밋

Leader와 Follower 모두 ISR(In-Sync Replica)이라는 논리 그룹으로 묶이게 된다. 이 그룹에 속하지 않는 Follower는 잠재적인 Leader의 자격을 얻지 못한다.

ISR 내 Follower들은 Leader와의 데이터 일치를 유지하기 위해 지속적으로 Leader의 데이터를 따라가게 되고 Leader는 ISR 내 모든 Follower가 Message를 받을 때까지 기다린다. (즉 Replication 동작을 잘 수행하는지 감시한다.)

만약 네트워크 혹은 Broker 장애로 뒤처진 Follower는 Leader에 의해서 ISR로부터 자격을 박탈당한다.

ISR 내 모든 Follower가 복제를 완료하면 Leader는 Commit 표시를 한다. (마지막 Commit Offset 위치를 High Watermark라고 부른다.) 이렇게 Commit된 Message만 Consumer가 읽어갈 수 있다. (이는 Message 일관성을 위해서임)

![High Watermark](https://i.sstatic.net/OYEFY.png)

#### Leader와 Follower의 단계별 Replication 동작

##### Leader 역할의 부하 분산

많은 Message의 읽기/쓰기 처리만도 벅찬 Leader가 Followers의 Replication 동작에도 관여를 하려면 너무 많은 성능의 손실이 발생하므로 서로 간의 통신 최소화가 필요하다.

RabbitMQ와 같은 Message Queue에서는 Kafka의 Follower격인 Mirror에서 Message를 받았음을 ACK 응답을 보내는 과정이 있으나 Kafka에서는 이러한 ACK 응답조차 제거하여 성능을 높임.

Last Commit Offset을 기준으로 Leader는 Follower들의 복제 요청 Commit Number를 확인하여 각 Follower가 어느 Offset까지 Replication을 성공했는지를 인지할 수 있다. 그리고 Leader의 Last Commit Offset을 Follower에 Message와 함께 응답하여 어디까지 Message가 Commit되었는지를 알림으로써 각 Follower는 Replication이 정상적으로 이루어지고 있는지를 평가할 수 있다.

> 이는 Leader -> Follower Push 방식이 아닌 Leader <- Follower Pull 방식으로 동작함을 의미한다.

##### Follower Fetch Request

![Follower Fetch Request](https://images.ctfassets.net/gt6dp23g0g38/QMNcHw9rAoiFGXj4DnP9I/51ef0ec74b91b1f01a88f8fa3934b2f0/Kafka_Internals_032.png)

Follower에서는 주기적으로 Leader에게 로그 복제를 위한 Fetch Request를 전송하며 여기에는 현재 Follower가 가지고 있는 Offset이 포함되어 있다.

##### Follower Fetch Response

![Follower Fetch Response](https://images.ctfassets.net/gt6dp23g0g38/7kr6K36N4VF4D5F3gpY71h/10329b5e4b700afdb1aa28a46432fd44/Kafka_Internals_033.png)

Leader는 Follower의 Fetch Request 내 Offset을 확인하고 해당 Offset 이후로 쌓인 데이터를 Fetch Response에 담아 전달한다. 이 과정을 통해 하나의 Follower에 데이터 복제가 이루어진다.

##### Committing Partition Offsets

![Committing Partition Offsets](https://images.ctfassets.net/gt6dp23g0g38/7ADIKF2poAYD0iE1p1hJNF/eff71842eed8637f50d888e27f962343/Kafka_Internals_034.png)

Leader는 모든 노드에 성공적으로 Message가 복제된 경우 해당 지점까지의 Offset을 커밋한다. 커밋이 완료되어야 비로소 Consumer가 해당 Partition에서 데이터 조회가 가능하다.

### 기존 High Watermark 방식의 Replication Protocol의 한계

**Leader Broker와 Follower Broker의 High Watermark 업데이트 시점에 Delay가 발생한다는 것이다.**

Leader는 Fetch Request를 전달받은 시점에 High Watermark를 업데이트하지만, Follower는 Fetch Response를 받은 시점에 High Watermark를 업데이트할 수 있기 때문이다.

업데이트 지연은 특정 상황에서 메시지 유실 뿐만 아니라 정합성 붕괴로도 이어진다.

#### Scenario 1] High Watermark Truncation followed by Immediate Leader Election

High Watermark 업데이트 지연이 발생한 Broker Node가 급작스럽게 Leader로 선출되는 경우

![High Watermark Truncation followed by Immediate Leader Election](https://velog.velcdn.com/images/koo8624/post/d58f6285-a2b8-45a3-8c48-de684f53b8ed/image.png)

이 시나리오는 Replicark 2개인 1 Partition을 가정한다.

- 복제를 마친 Follower Broker가 Leader Broker에 Fetch Request를 전송
- Leader는 Follower의 복제를 확인하고 High Watermark를 업데이트
- 업데이트된 High Watermark 정보를 담은 Fetch Response를 Follower에 전송
- Follower가 응답을 수신받지 못하고 재시작. Follower가 High Watermark 이후 데이터를 모두 지우고 Leader에 다시 Fetch Request를 전송
- Leader가 갑작스러운 장애로 Down
- Message 유실

이 사례는 Message 유실 뿐만 아니라 Phantom Read 같은 추가 문제도 발생시킨다. (Follower가 재시작되고 새로운 Leader가 되기 전까지 Message가 커밋된 상태로 Leader를 통해 Consumer에서 접근 가능했기 때문)

이러한 경우 2가지 해결책이 존재한다.

- Follower의 High Watermark 업데이트를 마칠 때까지 Leader의 High Watermark 업데이트를 지연시킨다.
- Follower가 재시작되면 로컬에 남아있는 High Watermark 이후 데이터를 즉시 삭제하지 않고 Leader에 Fetch Request를 전송하고 그 결과를 바탕으로 결정

첫번째 방법은 추가적인 RPC Call이 발생되 Kafka의 복제 프로토콜의 전반적 Latency를 크게 증가시키는 문제가 있다. 두번째 방법은 Message 유실 문제를 해결할 수 있지만 근본적으로 High Watermark에만 의존한 복구 방식은 아래 Scenario 2의 문제를 해결하기 어렵다.

#### Scenario 2] Replica Divergence on Restart after Multiple Hard Failures

모든 Broker가 동시에 종료되고, 불완전한 Follower Broker가 먼저 재시작되어 Leader가 되는 경우

![Replica Divergence on Restart after Multiple Hard Failures](https://velog.velcdn.com/images/koo8624/post/9da8ff33-dd1f-4e95-8eaf-ba5fc8cade0c/image.png)

그림에서 볼 수 있듯이 Broker간 서로 다른 메시지를 가지고 있지만 동일한 Offset을 가지고 있기 때문에 Topic의 붕괴로 이어질 수 있다.

---

### LeaderEpoch

![LeaderEpoch](https://velog.velcdn.com/images/koo8624/post/9c882285-30bb-4b58-818b-a25ca90c6d40/image.png)

LeaderEpoch는 현재 Partition Leader Broker에 대한 임시 식별자와 같은 개념으로 실제 0부터 시작하여 새로운 Leader 선출때마다 1씩 증가한다. 동일한 Partition의 동일한 Broker가 다시 Leader로 복귀한다 하더라도 LeaderEpoch는 이전 값과는 다른 값을 가지게 된다. 이러한 정보는 Controller에 의해 관리된다.

각 Broker는 Leader가 새롭게 선출될 때마다 LeaderEpoch Sequence (LeaderEpoch와 해당 시점의 Offset 조합)를 기록하고 복구 시에 High Watermark를 대신하여 이를 활용한다.

**Broker가 재시작되면 로컬에 기록된 High Watermark 이후의 데이터를 모두 삭제하지 않고 Leader에게 LeaderEpoch를 요청한 후 이를 비교해 삭제 여부를 판단한다.**

LeaderEpoch를 활용하면 앞서 나온 두 문제 모두 해결 가능하다.

#### (Again) Scenario 1] High Watermark Truncation followed by Immediate Leader Election

![(Again) Scenario 1\] High Watermark Truncation followed by Immediate Leader Election](https://velog.velcdn.com/images/koo8624/post/d500c3f8-8d78-4a0e-87b1-d1a8cd24ab12/image.png)

- Follower가 Leader의 데이터를 모두 복제하였으나 High Watermark를 업데이트하지 못한 채 종료
- 재시작된 Follower는 Leader에게 LeaderEpochRequest를 전송하고 현재 Offset에 해당하는 값을 받음
- Follower에 저장된 데이터 길이가 해당 Offset과 일치하므로 High Watermark와 무관하게 Message를 삭제하지 않음
- Leader가 장애로 종료
- Follower가 새 Leader로 선출되어 LeaderEpoch 값 증가되고 이를 Sequence에 기록

만약 기존 Leader Broker가 LeaderEpochRequest에 대한 응답을 주지 못하고 재시작되어도 결과는 동일함.

#### (Again) Scenario 2] Replica Divergence on Restart after Multiple Hard Failures

![(Again) Scenario 2\] Replica Divergence on Restart after Multiple Hard Failures](https://velog.velcdn.com/images/koo8624/post/656f271b-94e9-4a7e-809a-319b522497dd/image.png)

- 불완전하게 복제된 상태로 모든 Broker 종료
- Follower Broker가 먼저 재시작되어 새로운 Leader 선출
- Follower였던 Leader는 LeaderEpoch 교환을 하지 않음
- Producer로부터 새로운 Message를 전달받음
- 기존 Broker가 Follower로 재시작, LeaderEpochRequest를 전송하고 값을 반환받음
- 현제 로컬에 가지고 있는 Message Offset과 다르기 때문에 현재 Leader의 Start Offset 이후의 데이터가 모두 제거

LeaderEpoch 기반의 복구를 적용하면 불완전한 상태로 모든 Broker가 재시작되는 상황에서도 정합성 불일치가 발생하지 않음.

---

## Controller

Controller는 Leader Election 역할을 맡는다. Kafka Cluster에서 하나의 Broker가 Controller 역할을 맡으며, Partition의 ISR List에서 Leader를 선출한다. ISR List는 안전한 저장소 보관을 위해 Zookeeper에 저장한다.

> 최신 버전에서는 Zookeeper 의존성이 제거되어 KRaft로 이를 대체하고 있다.

## Log Segment

Kafka의 Topic으로 들어오는 모든 Message는 Segment 라고 하는 파일에 저장되며 Message는 정해진 형식에 맞추어 순차적으로 Segment 에 저장된다. 이 때 Message만 저장되는 것이 아니라 Message의 Key, Value, Offset, Size와 같은 정보가 함께 저장된다. Log Segment는 최대 크기가 1GB로 설정되어 있어 Rolling 전략을 사용하여 파일이 관리된다.
이러한 파일은 무제한으로 늘어나기 때문에 삭제 혹은 Compaction과 같은 관리 계획이 수립되어야 한다.

### 삭제

`server.properties` 에서 `log.cleanup.policy`가 `delete`로 지정되어야 한다. 기본적으로 이 옵션을 명시하지 않으면 활성화되어 있다고 보면 된다.

`retension.ms` 옵션 값에 따라 해당 시간이 지나면 삭제 작업이 진행되며 별도 지정하지 않는 경우 기본 값은 5분이다.

### Compaction

![Log Compaction](https://dz2cdn1.dzone.com/storage/temp/14018628-kafka-log-compaction-process.png)

로그를 삭제하지 않고 압축하는 방식이다. Message의 Key 값을 기준으로 마지막 데이터만 보관하는 형태이다.

이러한 방식은 과거 정보는 중요하지 않고 가장 마지막 값이 필요한 경우에 사용된다.

일반적으로 Kafka에서 Message를 Produce할 때 Key는 필수가 아니지만 Compaction 방식을 사용하고자 한다면 Key는 필수로 지정되어야 한다.

Compaction을 사용하면 장애 복구시 전체 로그를 복구하지 않고 Message Key를 기준으로 최신 상태만 복구하기 때문에 전체 로그를 복구할 때 대비 복구 시간이 줄어들어 빠른 장애 복구가 가능하다는 점이 있다.

빠른 재처리라는 장점이 있음에도 불구하고 모든 Topic에 Log Compaction을 적용하는 것이 좋은 것은 아님. Key 값을 기준으로 최종값만 필요한 워크로드에 적용하는 것이 바람직하다. 또한 Compaction 작업이 진행되는 동안 Broker의 과도한 입출력 부하가 발생하므로 Broker의 리소스 모니터링 또한 병행이 되어야 한다.

#### Log Compaction Related Properties

|Option|Value|적용 범위|Description|
|---|---|---|---|
|cleanup.policy|compact|Topic 옵션|Topic 레벨에서 Log Compaction을 설정할 때 적용|
|log.cleanup.policy|compact|Broker 설정 파일|Broker 레벨에서 Log Compaction을 설정할 때 적용|
|log.cleaner.min.compaction.lag.ms|0|Broker 설정 파일|Message 기록 후 Compaction 전 경과되어야할 최소 시간. 이 옵션이 설정되지 않으면 마지막 Segment를 제외하고 모든 Segment를 Compaction|
|log.cleaner.max.compaction.lag.ms|9223372036854775807|Broker 설정 파일|Message 기록 후 Compaction 전 경과되어야할 최대 시간|
|log.cleaner.min.cleanable.ratio|0.5|Broker 설정 파일|전체 로그 대비 로그에서 압축되지 않은 부분 (Dirty)의 비율이 해당 값을 넘으면 Log Compaction이 실행됨|
