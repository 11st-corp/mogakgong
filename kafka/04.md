# Kafka의 내부 동작 원리와 구현

---

## Replication

높은 가용성을 위해 제공되는 기능으로 각 Topic의 Partition들을 Kafka Cluster 내 다른 Broker로 복제하는 것을 의미한다. Topic을 생성할 때 `--replication-factor` 옵션으로 Replication 수를 지정해야 한다.

![Kafka Replication](https://images.ctfassets.net/gt6dp23g0g38/HZjoaXOuEc1zteyMcoOww/1ebab125a11552f4e8b8d88e7850f0ad/Kafka_Internals_029.png)

생성된 Replication은 `Leader`와 `Follower`로 나뉘며 `ISR (In-Sync Replica)` 이라는 일종의 Replication Group을 형성하여 관리된다.

### Leader / Follower

![Leader/Follower, ISR](https://images.ctfassets.net/gt6dp23g0g38/4Llth82ZvCCBqcHfp7v0lH/60e6f507fdccce263d38b6d285e6b143/Kafka_Internals_030.png)

Leader는 Replication 중 하나가 선정되며 모든 Read / Write Operation이 Leader를 통해서만 수행된다. 다시 말해 Producer와 Consumer의 Message 생성과 읽기는 모두 Leader를 통해서만 이루어진다.

Follower는 Leader에 문제가 발생할 경우를 대비해 새로운 Leader가 될 준비를 해야 한다. 따라서 지속적으로 Leader가 새로운 Message를 받았는지 확인하고 새로운 Message가 있는 경우 Leader로부터 해당 Message를 복제한다.

### 복제 유지와 커밋

Leader와 Follower 모두 ISR(In-Sync Replica)이라는 논리 그룹으로 묶이게 된다. 이 그룹에 속하지 않는 Follower는 잠재적인 Leader의 자격을 얻지 못한다.

ISR 내 Follower들은 Leader와의 데이터 일치를 유지하기 위해 지속적으로 Leader의 데이터를 따라가게 되고 Leader는 ISR 내 모든 Follower가 Message를 받을 때까지 기다린다. (즉 Replication 동작을 잘 수행하는지 감시한다.)

만약 네트워크 혹은 Broker 장애로 뒤처진 Follower는 Leader에 의해서 ISR로부터 자격을 박탈당한다.

ISR 내 모든 Follower가 복제를 완료하면 Leader는 Commit 표시를 한다. (마지막 Commit Offset 위치를 High Watermark라고 부른다.) 이렇게 Commit된 Message만 Consumer가 읽어갈 수 있다. (이는 Message 일관성을 위해서임)

![High Watermark](https://i.sstatic.net/OYEFY.png)

### Leader와 Follower의 단계별 Replication 동작

#### Leader 역할의 부하 분산

많은 Message의 읽기/쓰기 처리만도 벅찬 Leader가 Followers의 Replication 동작에도 관여를 하려면 너무 많은 성능의 손실이 발생하므로 서로 간의 통신 최소화가 필요하다.

RabbitMQ와 같은 Message Queue에서는 Kafka의 Follower격인 Mirror에서 Message를 받았음을 ACK 응답을 보내는 과정이 있으나 Kafka에서는 이러한 ACK 응답조차 제거하여 성능을 높임.

Last Commit Offset을 기준으로 Leader는 Follower들의 복제 요청 Commit Number를 확인하여 각 Follower가 어느 Offset까지 Replication을 성공했는지를 인지할 수 있다. 그리고 Leader의 Last Commit Offset을 Follower에 Message와 함께 응답하여 어디까지 Message가 Commit되었는지를 알림으로써 각 Follower는 Replication이 정상적으로 이루어지고 있는지를 평가할 수 있다.

> 이는 Leader -> Follower Push 방식이 아닌 Leader <- Follower Pull 방식으로 동작함을 의미한다.

#### Follower Fetch Request

![Follower Fetch Request](https://images.ctfassets.net/gt6dp23g0g38/QMNcHw9rAoiFGXj4DnP9I/51ef0ec74b91b1f01a88f8fa3934b2f0/Kafka_Internals_032.png)

#### Follower Fetch Response

![Follower Fetch Response](https://images.ctfassets.net/gt6dp23g0g38/7kr6K36N4VF4D5F3gpY71h/10329b5e4b700afdb1aa28a46432fd44/Kafka_Internals_033.png)

#### Partition Offset Commit

![Partition Offset Commit](https://images.ctfassets.net/gt6dp23g0g38/7ADIKF2poAYD0iE1p1hJNF/eff71842eed8637f50d888e27f962343/Kafka_Internals_034.png)

### LeaderEpoch와 복구

![LeaderEpoch](https://images.ctfassets.net/gt6dp23g0g38/1rHc9oqwn8DD94JIQckrXL/a36399e2acc2437810ddaa8a568307ca/Kafka_Internals_031.png)

각 Leader는 32bit의 고유하면서도 증가하는 숫자인 LeaderEpoch를 가지는데, 이는 Kafka의 Partition들이 복구 동작을 할 때 Message의 일관성을 유지하기 위한 용도로 이용된다. 위의 Fetch Request / Response에서 알 수 있듯 Offset 전달과 함께 LeaderEpoch 데이터가 함께 전달된다. **복구 동작이 필요한 경우 LeaderEpoch는 High Watermark를 대체하는 수단으로도 활용**된다.

> High Watermark란 ISR 내 모든 Follower의 복제가 완료되었을 때 Leader가 내부적으로 Commit되었다고 남기는 표시이다.
> ![High Watermark](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FwaLQG%2FbtrDyFSppqO%2FAzC3PQTwZH6JyE8ASnJPF0%2Fimg.png)
>
> 이러한 커밋 정보는 /data/kafka-logs/replication-offset-checkpoint 파일에서 확인 가능하다.

#### Leader Failure Case

![Leader Failure](https://images.ctfassets.net/gt6dp23g0g38/4gmUY2HRzEEgtWX4aYO5RK/92c1cd987a80a083e0903ab21bb7a6e6/Kafka_Internals_036.png)

Leader에 문제가 생겨 새 Leader Election이 필요한 경우 ISR 내 Broker 중 하나가 새로 선택되는데 이는 Control Plane (Controller) 에서 처리되며 새로 선출된 이후 LeaderEpoch가 증가되고 새로운 Leader가 Producer의 요청을 수락하기 시작한다.

#### Temporarily decreased High Watermark

![Temporarily decreased High Watermark](https://images.ctfassets.net/gt6dp23g0g38/Kr5GipOTKKo9KojdSmREF/a230a16ec35d8887e91cff2ddeb29e00/Kafka_Internals_037.png)

새로운 Leader가 선출된 직후 시점에서는 High Watermark가 실제 High Watermark보다 낮을 수 있기 때문에 Consumer에서 Offset 이후의 데이터 Fetch 요청에서 `OFFSET_NOT_AVAILABLE` 오류를 트리거할 수 있기 때문에 Consumer 쪽에서는 계속해서 Retry가 이루어지며, High Watermark가 업데이트되면 이 때부터는 정상적인 처리가 진행된다.

#### Partition Replica Reconciliation

![Partition Replica Reconciliation](https://images.ctfassets.net/gt6dp23g0g38/1MCX2GxiBgktyO7kPgSBGu/f0e8800cd5c4d66ec23243795b5597f5/Kafka_Internals_038.png)

새로운 Leader가 선택된 직후 일부 Follower의 Replication은 새로운 Leader와 동기화되지 않은 (커밋되지 못한) Message가 있을 수 있는데 이러한 문제가 해결될 때까지 Reconciliation 작업이 진행된다.

우선 동기화되지 않은 Follower가 Fetch Request를 보낸다.

#### Fetch Response Informs Follower of Divergence / Follower Truncates Log to Match Leader Log

![Fetch Response Informs Follower of Divergence](https://images.ctfassets.net/gt6dp23g0g38/1MCX2GxiBgktyO7kPgSBGu/1087c6850f64d0e30a86f97f8ac6f77a/Kafka_Internals_039.png)

Leader에서는 Fetch Request를 받으면 자체 로그와 비교해서 요청한 Offset이 해당 Epoch에 유효한지 여부를 확인해 해당 Epoch에 끝나야하는 Offset을 알려주는 응답을 Follower에 반환하고 Follower가 이를 바탕으로 정리 작업을 진행한다.

![Follower Truncates Log to Match Leader Log](https://images.ctfassets.net/gt6dp23g0g38/2SLCk6ccSIlkvPjrKq2YCi/492556be995fa402bd06e20cc24a6964/Kafka_Internals_040.png)

#### Subsequent Fetch with Updated Offset and Epoch

![Subsequent Fetch with Updated Offset and Epoch](https://images.ctfassets.net/gt6dp23g0g38/aYcacWtuT1gnS6RCQRxaa/5b2760f264a6fd99b29c16a03d030b03/Kafka_Internals_041.png)

#### Follower 102 Reconciled

![Follower 102 Reconciled](https://images.ctfassets.net/gt6dp23g0g38/5rtqIUVT29SGB5vharEdcE/ed09090d7e99d57752e001dfc8758d56/Kafka_Internals_042.png)

#### Follower 102 Acknowledges New Records

![Follower 102 Acknowledges New Records](https://images.ctfassets.net/gt6dp23g0g38/10rKaHrv3sJxZ9CxmYwL9w/b3a8d28f7b99a4b8cee79fe9a1b7697e/Kafka_Internals_043.png)

Reconciliation이 완료되어 Follower가 Leader의 Offset을 따라잡았을 경우 Leader에게 이를 알리고 Leader는 High Watermark를 높일 수 있게 된다.

#### Follower 101 Rejoins the Cluster

![Follower 101 Rejoins the Cluster](https://images.ctfassets.net/gt6dp23g0g38/slWIzdKdFUuiEwK1BAG4E/cd88e602396e4bcdacad99487ea4387f/Kafka_Internals_044.png)

과거에 Failure에 빠졌던 Old Leader가 다시 Online 상태로 돌아오면 마찬가지로 동일한 Reconciliation Process를 거친다. 이 작업이 완료되면 ISR에 다시 추가된다.

#### Handling Failed or Slow Followers

![Handling Failed or Slow Followers](https://images.ctfassets.net/gt6dp23g0g38/1T8pQOyW8YcUj64phgAYW5/c550c246504f246faab3f172f2f073a0/Kafka_Internals_045.png)

Leader가 Follower들의 진행 상황을 모니터하고, 이 과정에서 Follower가 Failure되는 경우 (마지막으로 완전히 따라잡힌 이후 구성 가능한 시간이 경과했을 때) ISR에서 제거한다. 다시 Online 상태가 되면 ISR에 재추가된다.

---

## Controller

Controller는 Leader Election 역할을 맡는다. Kafka Cluster에서 하나의 Broker가 Controller 역할을 맡으며, Partition의 ISR List에서 Leader를 선출한다. ISR List는 안전한 저장소 보관을 위해 Zookeeper에 저장한다.

> 최신 버전에서는 Zookeeper 의존성이 제거되어 KRaft로 이를 대체하고 있다.

## Log Segment

Kafka의 Topic으로 들어오는 모든 Message는 Segment 라고 하는 파일에 저장되며 Message는 정해진 형식에 맞추어 순차적으로 Segment 에 저장된다. 이 때 Message만 저장되는 것이 아니라 Message의 Key, Value, Offset, Size와 같은 정보가 함께 저장된다. Log Segment는 최대 크기가 1GB로 설정되어 있어 Rolling 전략을 사용하여 파일이 관리된다.
이러한 파일은 무제한으로 늘어나기 때문에 삭제 혹은 Compaction과 같은 관리 계획이 수립되어야 한다.

### 삭제

`server.properties` 에서 `log.cleanup.policy`가 `delete`로 지정되어야 한다. 기본적으로 이 옵션을 명시하지 않으면 활성화되어 있다고 보면 된다.

`retension.ms` 옵션 값에 따라 해당 시간이 지나면 삭제 작업이 진행되며 별도 지정하지 않는 경우 기본 값은 5분이다.

### Compaction

![Log Compaction](https://dz2cdn1.dzone.com/storage/temp/14018628-kafka-log-compaction-process.png)

로그를 삭제하지 않고 압축하는 방식이다. Message의 Key 값을 기준으로 마지막 데이터만 보관하는 형태이다.

이러한 방식은 과거 정보는 중요하지 않고 가장 마지막 값이 필요한 경우에 사용된다.

일반적으로 Kafka에서 Message를 Produce할 때 Key는 필수가 아니지만 Compaction 방식을 사용하고자 한다면 Key는 필수로 지정되어야 한다.

Compaction을 사용하면 장애 복구시 전체 로그를 복구하지 않고 Message Key를 기준으로 최신 상태만 복구하기 때문에 전체 로그를 복구할 때 대비 복구 시간이 줄어들어 빠른 장애 복구가 가능하다는 점이 있다.

빠른 재처리라는 장점이 있음에도 불구하고 모든 Topic에 Log Compaction을 적용하는 것이 좋은 것은 아님. Key 값을 기준으로 최종값만 필요한 워크로드에 적용하는 것이 바람직하다. 또한 Compaction 작업이 진행되는 동안 Broker의 과도한 입출력 부하가 발생하므로 Broker의 리소스 모니터링 또한 병행이 되어야 한다.

#### Log Compaction Related Properties

|Option|Value|적용 범위|Description|
|---|---|---|---|
|cleanup.policy|compact|Topic 옵션|Topic 레벨에서 Log Compaction을 설정할 때 적용|
|log.cleanup.policy|compact|Broker 설정 파일|Broker 레벨에서 Log Compaction을 설정할 때 적용|
|log.cleaner.min.compaction.lag.ms|0|Broker 설정 파일|Message 기록 후 Compaction 전 경과되어야할 최소 시간. 이 옵션이 설정되지 않으면 마지막 Segment를 제외하고 모든 Segment를 Compaction|
|log.cleaner.max.compaction.lag.ms|9223372036854775807|Broker 설정 파일|Message 기록 후 Compaction 전 경과되어야할 최대 시간|
|log.cleaner.min.cleanable.ratio|0.5|Broker 설정 파일|전체 로그 대비 로그에서 압축되지 않은 부분 (Dirty)의 비율이 해당 값을 넘으면 Log Compaction이 실행됨|
